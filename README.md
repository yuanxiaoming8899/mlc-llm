<div class="Box-sc-g0xbh4-0 bJMeLZ js-snippet-clipboard-copy-unpositioned" data-hpc="true"><article class="markdown-body entry-content container-lg" itemprop="text"><div class="markdown-heading" dir="auto"><h1 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">法学硕士</font></font></h1><a id="user-content-mlc-llm" class="anchor" aria-label="永久链接：MLC 法学硕士" href="#mlc-llm"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><a href="https://llm.mlc.ai/docs" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">文档</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">|</font></font><a href="https://blog.mlc.ai/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">博客</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">|</font></font><a href="https://discord.gg/9Xpy2HGBuD" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">不和谐</font></font></a></p>
<p dir="auto"><strong><font style="vertical-align: inherit;"></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">大型</font><strong><font style="vertical-align: inherit;">语言</font></strong><strong><font style="vertical-align: inherit;">模型</font></strong><font style="vertical-align: inherit;">的</font><strong><font style="vertical-align: inherit;">机器</font></strong></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">学习</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">编译</font><font style="vertical-align: inherit;">(MLC LLM) 是一种高性能通用部署解决方案，允许使用具有编译器加速功能的本机 API 来本机部署任何</font></font><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">大型</font></font></strong><font style="vertical-align: inherit;"><strong><font style="vertical-align: inherit;">语言</font></strong><font style="vertical-align: inherit;">模型</font><font style="vertical-align: inherit;">。</font><font style="vertical-align: inherit;">该项目的使命是让每个人都能够利用机器学习编译技术在每个人的设备上本地开发、优化和部署人工智能模型。</font></font><strong><font style="vertical-align: inherit;"></font></strong><font style="vertical-align: inherit;"></font><strong><font style="vertical-align: inherit;"></font></strong><font style="vertical-align: inherit;"></font><strong><font style="vertical-align: inherit;"></font></strong><font style="vertical-align: inherit;"></font></p>
<p dir="auto"><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通用部署。</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;"> MLC LLM 支持以下平台和硬件：</font></font></p>
<table>
  <thead>
    <tr>
      <th> </th>
      <th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">AMD显卡</font></font></th>
      <th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">英伟达图形处理器</font></font></th>
      <th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">苹果GPU</font></font></th>
      <th><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">英特尔GPU</font></font></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Linux / 操作系统</font></font></td>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">✅ Vulkan、ROCm</font></font></td>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">✅ 伏尔甘、CUDA</font></font></td>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">不适用</font></font></td>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">✅ 伏尔甘</font></font></td>
    </tr>
    <tr>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">苹果系统</font></font></td>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">✅ 金属 (dGPU)</font></font></td>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">不适用</font></font></td>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">✅ 金属</font></font></td>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">✅ 金属（iGPU）</font></font></td>
    </tr>
    <tr>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">网页浏览器</font></font></td>
      <td colspan="4"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">✅ WebGPU 和 WASM</font></font></td>
    </tr>
    <tr>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">iOS/iPadOS</font></font></td>
      <td colspan="4"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">✅ Apple A 系列 GPU 上的 Metal</font></font></td>
    </tr>
    <tr>
      <td><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">安卓</font></font></td>
      <td colspan="2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">✅ Adreno GPU 上的 OpenCL</font></font></td>
      <td colspan="2"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">✅ Mali GPU 上的 OpenCL</font></font></td>
    </tr>
  </tbody>
</table>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">快速开始</font></font></h2><a id="user-content-quick-start" class="anchor" aria-label="永久链接：快速入门" href="#quick-start"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我们在这里介绍聊天 CLI、Python API 和 REST 服务器的快速入门示例，以使用 MLC LLM。我们使用 4 位量化 8B Llama-3 模型进行演示。预量化的 Llama-3 权重可在</font></font><a href="https://huggingface.co/mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">https://huggingface.co/mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">上获取。您还可以通过替换</font><font style="vertical-align: inherit;">下面示例中的</font></font><code>q4f16_1</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">to</font><font style="vertical-align: inherit;">来尝试未量化的 Llama-3 模型。</font></font><code>q0f16</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">请访问我们的</font></font><a href="https://llm.mlc.ai/docs/index.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">文档</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">以获取详细的快速入门和介绍。</font></font></p>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">安装</font></font></h3><a id="user-content-installation" class="anchor" aria-label="永久链接：安装" href="#installation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MLC LLM 可通过</font></font><a href="https://llm.mlc.ai/docs/install/mlc_llm.html#install-mlc-packages" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">pip</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">获得。始终建议将其安装在隔离的 conda 虚拟环境中。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">要验证安装，请激活您的虚拟环境，运行</font></font></p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto"><pre>python -c <span class="pl-s"><span class="pl-pds">"</span>import mlc_llm; print(mlc_llm.__path__)<span class="pl-pds">"</span></span></pre><div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w" value="python -c &quot;import mlc_llm; print(mlc_llm.__path__)&quot;" tabindex="0" role="button">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">您应该看到 MLC LLM Python 包的安装路径。</font></font></p>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">聊天 CLI</font></font></h3><a id="user-content-chat-cli" class="anchor" aria-label="永久链接：聊天 CLI" href="#chat-cli"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我们可以使用 4 位量化 8B Llama-3 模型尝试 MLC LLM 中的聊天 CLI。</font></font></p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto"><pre>mlc_llm chat HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC</pre><div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w" value="mlc_llm chat HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC" tabindex="0" role="button">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">第一次运行该命令可能需要1-2分钟。等待后，此命令会启动一个聊天界面，您可以在其中输入提示并与模型聊天。</font></font></p>
<div class="snippet-clipboard-content notranslate position-relative overflow-auto"><pre class="notranslate"><code>You can use the following special commands:
/help               print the special commands
/exit               quit the cli
/stats              print out the latest stats (token/sec)
/reset              restart a fresh chat
/set [overrides]    override settings in the generation config. For example,
                      `/set temperature=0.5;max_gen_len=100;stop=end,stop`
                      Note: Separate stop words in the `stop` option with commas (,).
Multi-line input: Use escape+enter to start a new line.

user: What's the meaning of life
assistant:
What a profound and intriguing question! While there's no one definitive answer, I'd be happy to help you explore some perspectives on the meaning of life.

The concept of the meaning of life has been debated and...
</code></pre><div class="zeroclipboard-container">
  
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div></div>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Python API</font></font></h3><a id="user-content-python-api" class="anchor" aria-label="永久链接：Python API" href="#python-api"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我们可以使用 MLC LLM 的聊天完成 Python API 来运行 Llama-3 模型。您可以将下面的代码保存到 Python 文件中并运行它。</font></font></p>
<div class="highlight highlight-source-python notranslate position-relative overflow-auto" dir="auto"><pre><span class="pl-k">from</span> <span class="pl-s1">mlc_llm</span> <span class="pl-k">import</span> <span class="pl-v">MLCEngine</span>

<span class="pl-c"># Create engine</span>
<span class="pl-s1">model</span> <span class="pl-c1">=</span> <span class="pl-s">"HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC"</span>
<span class="pl-s1">engine</span> <span class="pl-c1">=</span> <span class="pl-v">MLCEngine</span>(<span class="pl-s1">model</span>)

<span class="pl-c"># Run chat completion in OpenAI API.</span>
<span class="pl-k">for</span> <span class="pl-s1">response</span> <span class="pl-c1">in</span> <span class="pl-s1">engine</span>.<span class="pl-s1">chat</span>.<span class="pl-s1">completions</span>.<span class="pl-en">create</span>(
    <span class="pl-s1">messages</span><span class="pl-c1">=</span>[{<span class="pl-s">"role"</span>: <span class="pl-s">"user"</span>, <span class="pl-s">"content"</span>: <span class="pl-s">"What is the meaning of life?"</span>}],
    <span class="pl-s1">model</span><span class="pl-c1">=</span><span class="pl-s1">model</span>,
    <span class="pl-s1">stream</span><span class="pl-c1">=</span><span class="pl-c1">True</span>,
):
    <span class="pl-k">for</span> <span class="pl-s1">choice</span> <span class="pl-c1">in</span> <span class="pl-s1">response</span>.<span class="pl-s1">choices</span>:
        <span class="pl-en">print</span>(<span class="pl-s1">choice</span>.<span class="pl-s1">delta</span>.<span class="pl-s1">content</span>, <span class="pl-s1">end</span><span class="pl-c1">=</span><span class="pl-s">""</span>, <span class="pl-s1">flush</span><span class="pl-c1">=</span><span class="pl-c1">True</span>)
<span class="pl-en">print</span>(<span class="pl-s">"<span class="pl-cce">\n</span>"</span>)

<span class="pl-s1">engine</span>.<span class="pl-en">terminate</span>()</pre><div class="zeroclipboard-container">
   

# Create engine
model = &quot;HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC&quot;
engine = MLCEngine(model)

# Run chat completion in OpenAI API.
for response in engine.chat.completions.create(
    messages=[{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the meaning of life?&quot;}],
    model=model,
    stream=True,
):
    for choice in response.choices:
        print(choice.delta.content, end=&quot;&quot;, flush=True)
print(&quot;\n&quot;)

engine.terminate()" tabindex="0" role="button">
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div></div>
<p dir="auto"><strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Python API</font></font><code>mlc_llm.MLCEngine</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">与 OpenAI API 完全一致</font></font></strong><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">。您可以像使用</font></font><a href="https://github.com/openai/openai-python?tab=readme-ov-file#usage"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">OpenAI 的 Python 包</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">一样使用 MLCEngine
</font><font style="vertical-align: inherit;">
进行同步和异步生成。</font></font></p>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">如果您想进行并发异步生成，</font></font><code>mlc_llm.AsyncMLCEngine</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">则可以使用。</font></font></p>
<div class="markdown-heading" dir="auto"><h3 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">休息服务器</font></font></h3><a id="user-content-rest-server" class="anchor" aria-label="永久链接：REST 服务器" href="#rest-server"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">我们可以启动一个 REST 服务器来为 OpenAI 聊天完成请求的 4 位量化 Llama-3 模型提供服务。该服务器具有完全的 OpenAI API 完整性。</font></font></p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto"><pre>mlc_llm serve HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC</pre><div class="zeroclipboard-container">
   
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-copy js-clipboard-copy-icon">
    <path d="M0 6.75C0 5.784.784 5 1.75 5h1.5a.75.75 0 0 1 0 1.5h-1.5a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-1.5a.75.75 0 0 1 1.5 0v1.5A1.75 1.75 0 0 1 9.25 16h-7.5A1.75 1.75 0 0 1 0 14.25Z"></path><path d="M5 1.75C5 .784 5.784 0 6.75 0h7.5C15.216 0 16 .784 16 1.75v7.5A1.75 1.75 0 0 1 14.25 11h-7.5A1.75 1.75 0 0 1 5 9.25Zm1.75-.25a.25.25 0 0 0-.25.25v7.5c0 .138.112.25.25.25h7.5a.25.25 0 0 0 .25-.25v-7.5a.25.25 0 0 0-.25-.25Z"></path>
</svg>
      <svg aria-hidden="true" height="16" viewBox="0 0 16 16" version="1.1" width="16" data-view-component="true" class="octicon octicon-check js-clipboard-check-icon color-fg-success d-none">
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">默认情况下已连接服务器</font></font><code>http://127.0.0.1:8000</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，您可以使用</font></font><code>--host</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">和</font></font><code>--port</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">
设置不同的主机和端口。当服务器准备就绪（显示</font></font><code>INFO: Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)</code><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">）时，我们可以打开一个新 shell 并通过以下命令发送 cURL 请求：</font></font></p>
<div class="highlight highlight-source-shell notranslate position-relative overflow-auto" dir="auto"><pre>curl -X POST \
  -H <span class="pl-s"><span class="pl-pds">"</span>Content-Type: application/json<span class="pl-pds">"</span></span> \
  -d <span class="pl-s"><span class="pl-pds">'</span>{</span>
<span class="pl-s">        "model": "HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC",</span>
<span class="pl-s">        "messages": [</span>
<span class="pl-s">            {"role": "user", "content": "Hello! Our project is MLC LLM. What is the name of our project?"}</span>
<span class="pl-s">        ]</span>
<span class="pl-s">  }<span class="pl-pds">'</span></span> \
  http://127.0.0.1:8000/v1/chat/completions</pre><div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w" value="curl -X POST \
  -H &quot;Content-Type: application/json&quot; \
  -d '{
        &quot;model&quot;: &quot;HF://mlc-ai/Llama-3-8B-Instruct-q4f16_1-MLC&quot;,
        &quot;messages&quot;: [
            {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Hello! Our project is MLC LLM. What is the name of our project?&quot;}
        ]
  }' \
  http://127.0.0.1:8000/v1/chat/completions" tabindex="0" role="button">
    
</svg>
    </clipboard-copy>
  </div></div>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">通用部署 API</font></font></h2><a id="user-content-universal-deployment-apis" class="anchor" aria-label="永久链接：通用部署 API" href="#universal-deployment-apis"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MLC LLM 提供多组跨平台和环境的 API。这些包括</font></font></p>
<ul dir="auto">
<li><a href="https://llm.mlc.ai/docs/deploy/python_engine.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Python API</font></font></a></li>
<li><a href="https://llm.mlc.ai/docs/deploy/rest.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">兼容 OpenAI 的 Rest-API</font></font></a></li>
<li><a href="https://llm.mlc.ai/docs/deploy/cli.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">C++ API</font></font></a></li>
<li><a href="https://llm.mlc.ai/docs/deploy/javascript.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">JavaScript API</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">和</font></font><a href="https://github.com/mlc-ai/web-llm"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Web LLM</font></font></a></li>
<li><a href="https://llm.mlc.ai/docs/deploy/ios.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">适用于 iOS 应用程序的 Swift API</font></font></a></li>
<li><a href="https://llm.mlc.ai/docs/deploy/android.html" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">Java API 和 Android 应用程序</font></font></a></li>
</ul>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">引文</font></font></h2><a id="user-content-citation" class="anchor" aria-label="永久链接：引文" href="#citation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">如果您觉得有用，请考虑引用我们的项目：</font></font></p>
<div class="highlight highlight-text-bibtex notranslate position-relative overflow-auto" dir="auto"><pre><span class="pl-k">@software</span>{<span class="pl-en">mlc-llm</span>,
    <span class="pl-s">author</span> = <span class="pl-s"><span class="pl-pds">{</span>MLC team<span class="pl-pds">}</span></span>,
    <span class="pl-s">title</span> = <span class="pl-s"><span class="pl-pds">{</span>{MLC-LLM}<span class="pl-pds">}</span></span>,
    <span class="pl-s">url</span> = <span class="pl-s"><span class="pl-pds">{</span>https://github.com/mlc-ai/mlc-llm<span class="pl-pds">}</span></span>,
    <span class="pl-s">year</span> = <span class="pl-s"><span class="pl-pds">{</span>2023<span class="pl-pds">}</span></span>
}</pre><div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w" value="@software{mlc-llm,
    author = {MLC team},
    title = {{MLC-LLM}},
    url = {https://github.com/mlc-ai/mlc-llm},
    year = {2023}
}" tabindex="0" role="button">
     
</svg>
    </clipboard-copy>
  </div></div>
<p dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">MLC LLM的基础技术包括：</font></font></p>
<details>
  <summary><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">参考文献（点击展开）</font></font></summary>
<div class="highlight highlight-text-bibtex notranslate position-relative overflow-auto" dir="auto"><pre><span class="pl-k">@inproceedings</span>{<span class="pl-en">tensorir</span>,
    <span class="pl-s">author</span> = <span class="pl-s"><span class="pl-pds">{</span>Feng, Siyuan and Hou, Bohan and Jin, Hongyi and Lin, Wuwei and Shao, Junru and Lai, Ruihang and Ye, Zihao and Zheng, Lianmin and Yu, Cody Hao and Yu, Yong and Chen, Tianqi<span class="pl-pds">}</span></span>,
    <span class="pl-s">title</span> = <span class="pl-s"><span class="pl-pds">{</span>TensorIR: An Abstraction for Automatic Tensorized Program Optimization<span class="pl-pds">}</span></span>,
    <span class="pl-s">year</span> = <span class="pl-s"><span class="pl-pds">{</span>2023<span class="pl-pds">}</span></span>,
    <span class="pl-s">isbn</span> = <span class="pl-s"><span class="pl-pds">{</span>9781450399166<span class="pl-pds">}</span></span>,
    <span class="pl-s">publisher</span> = <span class="pl-s"><span class="pl-pds">{</span>Association for Computing Machinery<span class="pl-pds">}</span></span>,
    <span class="pl-s">address</span> = <span class="pl-s"><span class="pl-pds">{</span>New York, NY, USA<span class="pl-pds">}</span></span>,
    <span class="pl-s">url</span> = <span class="pl-s"><span class="pl-pds">{</span>https://doi.org/10.1145/3575693.3576933<span class="pl-pds">}</span></span>,
    <span class="pl-s">doi</span> = <span class="pl-s"><span class="pl-pds">{</span>10.1145/3575693.3576933<span class="pl-pds">}</span></span>,
    <span class="pl-s">booktitle</span> = <span class="pl-s"><span class="pl-pds">{</span>Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2<span class="pl-pds">}</span></span>,
    <span class="pl-s">pages</span> = <span class="pl-s"><span class="pl-pds">{</span>804–817<span class="pl-pds">}</span></span>,
    <span class="pl-s">numpages</span> = <span class="pl-s"><span class="pl-pds">{</span>14<span class="pl-pds">}</span></span>,
    <span class="pl-s">keywords</span> = <span class="pl-s"><span class="pl-pds">{</span>Tensor Computation, Machine Learning Compiler, Deep Neural Network<span class="pl-pds">}</span></span>,
    <span class="pl-s">location</span> = <span class="pl-s"><span class="pl-pds">{</span>Vancouver, BC, Canada<span class="pl-pds">}</span></span>,
    <span class="pl-s">series</span> = <span class="pl-s"><span class="pl-pds">{</span>ASPLOS 2023<span class="pl-pds">}</span></span>
}

<span class="pl-k">@inproceedings</span>{<span class="pl-en">metaschedule</span>,
    <span class="pl-s">author</span> = <span class="pl-s"><span class="pl-pds">{</span>Shao, Junru and Zhou, Xiyou and Feng, Siyuan and Hou, Bohan and Lai, Ruihang and Jin, Hongyi and Lin, Wuwei and Masuda, Masahiro and Yu, Cody Hao and Chen, Tianqi<span class="pl-pds">}</span></span>,
    <span class="pl-s">booktitle</span> = <span class="pl-s"><span class="pl-pds">{</span>Advances in Neural Information Processing Systems<span class="pl-pds">}</span></span>,
    <span class="pl-s">editor</span> = <span class="pl-s"><span class="pl-pds">{</span>S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh<span class="pl-pds">}</span></span>,
    <span class="pl-s">pages</span> = <span class="pl-s"><span class="pl-pds">{</span>35783--35796<span class="pl-pds">}</span></span>,
    <span class="pl-s">publisher</span> = <span class="pl-s"><span class="pl-pds">{</span>Curran Associates, Inc.<span class="pl-pds">}</span></span>,
    <span class="pl-s">title</span> = <span class="pl-s"><span class="pl-pds">{</span>Tensor Program Optimization with Probabilistic Programs<span class="pl-pds">}</span></span>,
    <span class="pl-s">url</span> = <span class="pl-s"><span class="pl-pds">{</span>https://proceedings.neurips.cc/paper_files/paper/2022/file/e894eafae43e68b4c8dfdacf742bcbf3-Paper-Conference.pdf<span class="pl-pds">}</span></span>,
    <span class="pl-s">volume</span> = <span class="pl-s"><span class="pl-pds">{</span>35<span class="pl-pds">}</span></span>,
    <span class="pl-s">year</span> = <span class="pl-s"><span class="pl-pds">{</span>2022<span class="pl-pds">}</span></span>
}

<span class="pl-k">@inproceedings</span>{<span class="pl-en">tvm</span>,
    <span class="pl-s">author</span> = <span class="pl-s"><span class="pl-pds">{</span>Tianqi Chen and Thierry Moreau and Ziheng Jiang and Lianmin Zheng and Eddie Yan and Haichen Shen and Meghan Cowan and Leyuan Wang and Yuwei Hu and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy<span class="pl-pds">}</span></span>,
    <span class="pl-s">title</span> = <span class="pl-s"><span class="pl-pds">{</span>{TVM}: An Automated {End-to-End} Optimizing Compiler for Deep Learning<span class="pl-pds">}</span></span>,
    <span class="pl-s">booktitle</span> = <span class="pl-s"><span class="pl-pds">{</span>13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)<span class="pl-pds">}</span></span>,
    <span class="pl-s">year</span> = <span class="pl-s"><span class="pl-pds">{</span>2018<span class="pl-pds">}</span></span>,
    <span class="pl-s">isbn</span> = <span class="pl-s"><span class="pl-pds">{</span>978-1-939133-08-3<span class="pl-pds">}</span></span>,
    <span class="pl-s">address</span> = <span class="pl-s"><span class="pl-pds">{</span>Carlsbad, CA<span class="pl-pds">}</span></span>,
    <span class="pl-s">pages</span> = <span class="pl-s"><span class="pl-pds">{</span>578--594<span class="pl-pds">}</span></span>,
    <span class="pl-s">url</span> = <span class="pl-s"><span class="pl-pds">{</span>https://www.usenix.org/conference/osdi18/presentation/chen<span class="pl-pds">}</span></span>,
    <span class="pl-s">publisher</span> = <span class="pl-s"><span class="pl-pds">{</span>USENIX Association<span class="pl-pds">}</span></span>,
    <span class="pl-s">month</span> = oct,
}</pre><div class="zeroclipboard-container">
    <clipboard-copy aria-label="Copy" class="ClipboardButton btn btn-invisible js-clipboard-copy m-2 p-0 tooltipped-no-delay d-flex flex-justify-center flex-items-center" data-copy-feedback="Copied!" data-tooltip-direction="w" value="@inproceedings{tensorir,
    author = {Feng, Siyuan and Hou, Bohan and Jin, Hongyi and Lin, Wuwei and Shao, Junru and Lai, Ruihang and Ye, Zihao and Zheng, Lianmin and Yu, Cody Hao and Yu, Yong and Chen, Tianqi},
    title = {TensorIR: An Abstraction for Automatic Tensorized Program Optimization},
    year = {2023},
    isbn = {9781450399166},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3575693.3576933},
    doi = {10.1145/3575693.3576933},
    booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
    pages = {804–817},
    numpages = {14},
    keywords = {Tensor Computation, Machine Learning Compiler, Deep Neural Network},
    location = {Vancouver, BC, Canada},
    series = {ASPLOS 2023}
}

@inproceedings{metaschedule,
    author = {Shao, Junru and Zhou, Xiyou and Feng, Siyuan and Hou, Bohan and Lai, Ruihang and Jin, Hongyi and Lin, Wuwei and Masuda, Masahiro and Yu, Cody Hao and Chen, Tianqi},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
    pages = {35783--35796},
    publisher = {Curran Associates, Inc.},
    title = {Tensor Program Optimization with Probabilistic Programs},
    url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/e894eafae43e68b4c8dfdacf742bcbf3-Paper-Conference.pdf},
    volume = {35},
    year = {2022}
}

@inproceedings{tvm,
    author = {Tianqi Chen and Thierry Moreau and Ziheng Jiang and Lianmin Zheng and Eddie Yan and Haichen Shen and Meghan Cowan and Leyuan Wang and Yuwei Hu and Luis Ceze and Carlos Guestrin and Arvind Krishnamurthy},
    title = {{TVM}: An Automated {End-to-End} Optimizing Compiler for Deep Learning},
    booktitle = {13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
    year = {2018},
    isbn = {978-1-939133-08-3},
    address = {Carlsbad, CA},
    pages = {578--594},
    url = {https://www.usenix.org/conference/osdi18/presentation/chen},
    publisher = {USENIX Association},
    month = oct,
}" tabindex="0" role="button">
    
    <path d="M13.78 4.22a.75.75 0 0 1 0 1.06l-7.25 7.25a.75.75 0 0 1-1.06 0L2.22 9.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018L6 10.94l6.72-6.72a.75.75 0 0 1 1.06 0Z"></path>
</svg>
    </clipboard-copy>
  </div></div>
</details>
<div class="markdown-heading" dir="auto"><h2 tabindex="-1" class="heading-element" dir="auto"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">链接</font></font></h2><a id="user-content-links" class="anchor" aria-label="永久链接： 链接" href="#links"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path d="m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z"></path></svg></a></div>
<ul dir="auto">
<li><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">您可能想查看我们的在线公共</font></font><a href="https://mlc.ai" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">机器学习编译课程</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">，以系统地了解我们的方法。</font></font></li>
<li><a href="https://webllm.mlc.ai/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WebLLM</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">是一个使用 MLC LLM 的 WebGPU 和 WebAssembly 后端的配套项目。</font></font></li>
<li><a href="https://websd.mlc.ai/" rel="nofollow"><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">WebStableDiffusion</font></font></a><font style="vertical-align: inherit;"><font style="vertical-align: inherit;">是具有 WebGPU 后端的扩散模型的配套项目。</font></font></li>
</ul>
</article></div>
